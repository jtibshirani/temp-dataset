def elbow_method(data, k_min, k_max, distance='euclidean'):
        """
        Calculates and plots the plot of variance explained - number of clusters
        Implementation reference: https://github.com/sarguido/k-means-clustering.rst

        :param data: The dataset
        :param k_min: lowerbound of the cluster range
        :param k_max: upperbound of the cluster range
        :param distance: the distance metric, 'euclidean' by default
        :return:
        """
        # Determine your k range
        k_range = range(k_min, k_max)

        # Fit the kmeans model for each n_clusters = k
        k_means_var = [Clustering.kmeans(k).fit(data) for k in k_range]

        # Pull out the cluster centers for each model
        centroids = [X.model.cluster_centers_ for X in k_means_var]

        # Calculate the Euclidean distance from
        # each point to each cluster center
        k_euclid = [cdist(data, cent, distance) for cent in centroids]
        dist = [np.min(ke, axis=1) for ke in k_euclid]

        # Total within-cluster sum of squares
        wcss = [sum(d ** 2) for d in dist]

        # The total sum of squares
        tss = sum(pdist(data) ** 2) / data.shape[0]

        # The between-cluster sum of squares
        bss = tss - wcss

        # elbow curve
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.plot(k_range, bss / tss * 100, 'b*-')
        ax.set_ylim((0, 100))
        plt.grid(True)
        plt.xlabel('n_clusters')
        plt.ylabel('Percentage of variance explained')
        plt.title('Variance Explained vs. k')
        plt.show()